{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-17T15:57:35.375095Z","iopub.status.busy":"2024-05-17T15:57:35.374741Z","iopub.status.idle":"2024-05-17T15:57:49.052140Z","shell.execute_reply":"2024-05-17T15:57:49.050842Z","shell.execute_reply.started":"2024-05-17T15:57:35.375065Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'Emotion-Classification' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/AminMohamed-3/Emotion-Classification.git\n","!pip install transformers dataset accelerate -q\n","import sys\n","sys.path.append(\"/kaggle/working/Emotion-Classification\")"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:57:49.054802Z","iopub.status.busy":"2024-05-17T15:57:49.054440Z","iopub.status.idle":"2024-05-17T15:57:51.458996Z","shell.execute_reply":"2024-05-17T15:57:51.458062Z","shell.execute_reply.started":"2024-05-17T15:57:49.054770Z"},"trusted":true},"outputs":[],"source":["import torch\n","from Training.dataset import prepare_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    DataCollatorForTokenClassification,\n","    Trainer,\n",")\n","import numpy as np\n","from config import NUM_LABELS\n","import wandb\n","from Training.utils import compute_metrics\n","from Training.utils import MultiLabelTrainer"]},{"cell_type":"markdown","metadata":{},"source":["# Define the model & Prepare Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["model_checkpoint = \"FacebookAI/roberta-base\" # Using a larger model\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","dataset, id2label, label2id = prepare_dataset(tokenizer)\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id, problem_type=\"multi_label_classification\"\n",")\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:58:21.233664Z","iopub.status.busy":"2024-05-17T15:58:21.232988Z","iopub.status.idle":"2024-05-17T15:58:23.554234Z","shell.execute_reply":"2024-05-17T15:58:23.553437Z","shell.execute_reply.started":"2024-05-17T15:58:21.233627Z"},"metadata":{},"trusted":true},"outputs":[],"source":["#data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n","\n","# Define the training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=3,  # Training for longer\n","    per_device_train_batch_size=16, # smaller batch size\n","    per_device_eval_batch_size=32,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5, # Using a smaller LR\n","    save_total_limit=10,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    logging_steps=20,\n","    # weight decay\n","    weight_decay=0.01,\n","    #lr_scheduler_type='cosine',\n","    #warmup_ratio=0.1,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:29:32.406005Z","iopub.status.busy":"2024-05-17T16:29:32.404986Z","iopub.status.idle":"2024-05-17T16:29:32.422235Z","shell.execute_reply":"2024-05-17T16:29:32.421204Z","shell.execute_reply.started":"2024-05-17T16:29:32.405968Z"},"metadata":{},"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"val\"],\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maminmoe707\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/amin/.netrc\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/mnt/c/Data/Emotion Classification/wandb/run-20240518_162535-3tcnz7vm</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/aminmoe707/huggingface/runs/3tcnz7vm' target=\"_blank\">eager-music-44</a></strong> to <a href='https://wandb.ai/aminmoe707/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/aminmoe707/huggingface' target=\"_blank\">https://wandb.ai/aminmoe707/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/aminmoe707/huggingface/runs/3tcnz7vm' target=\"_blank\">https://wandb.ai/aminmoe707/huggingface/runs/3tcnz7vm</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4' max='31686' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [    4/31686 00:14 < 62:27:28, 0.14 it/s, Epoch 0.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m62f8ddd1a44f05efc5c27f0ee5f22cf5bd70abc5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/main/lib/python3.12/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/main/lib/python3.12/site-packages/transformers/trainer.py:1966\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["wandb.login(key=\"62f8ddd1a44f05efc5c27f0ee5f22cf5bd70abc5\")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
