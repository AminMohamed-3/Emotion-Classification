{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/AminMohamed-3/Emotion-Classification.git\n!pip install transformers dataset accelerate -q\nimport sys\nsys.path.append(\"/kaggle/working/Emotion-Classification\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-17T15:57:35.375095Z","iopub.status.busy":"2024-05-17T15:57:35.374741Z","iopub.status.idle":"2024-05-17T15:57:49.052140Z","shell.execute_reply":"2024-05-17T15:57:49.050842Z","shell.execute_reply.started":"2024-05-17T15:57:35.375065Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"fatal: destination path 'Emotion-Classification' already exists and is not an empty directory.\n"}]},{"cell_type":"code","source":"import torch\nfrom Training.dataset import prepare_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    DataCollatorForTokenClassification\n)\nimport numpy as np\nfrom config import NUM_LABELS\nimport wandb\nfrom Training.utils import compute_metrics\nfrom Training.utils import MultiLabelTrainer","metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:57:49.054802Z","iopub.status.busy":"2024-05-17T15:57:49.054440Z","iopub.status.idle":"2024-05-17T15:57:51.458996Z","shell.execute_reply":"2024-05-17T15:57:51.458062Z","shell.execute_reply.started":"2024-05-17T15:57:49.054770Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Define the model & Prepare Dataset","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"FacebookAI/roberta-base\" # Using a larger model\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\ndataset, id2label, label2id = prepare_dataset(tokenizer)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_checkpoint, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id\n)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)","metadata":{"metadata":{}},"execution_count":2,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 211225/211225 [00:01<00:00, 132837.53it/s]\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9a5c48835524612a4586aebf7cfa6e2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/168980 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3143b991ead142a5856caf21bec9b05e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/21122 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c8c32d6cca74971a89df9ddd4bcf5c0","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/21123 [00:00<?, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"}]},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=10,  # Training for longer\n    per_device_train_batch_size=16, # smaller batch size\n    per_device_eval_batch_size=32,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5, # Using a smaller LR\n    save_total_limit=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    logging_steps=20,\n    # weight decay\n    weight_decay=0.01,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.1,\n)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:58:21.233664Z","iopub.status.busy":"2024-05-17T15:58:21.232988Z","iopub.status.idle":"2024-05-17T15:58:23.554234Z","shell.execute_reply":"2024-05-17T15:58:23.553437Z","shell.execute_reply.started":"2024-05-17T15:58:21.233627Z"},"metadata":{},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"trainer = MultiLabelTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"val\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:29:32.406005Z","iopub.status.busy":"2024-05-17T16:29:32.404986Z","iopub.status.idle":"2024-05-17T16:29:32.422235Z","shell.execute_reply":"2024-05-17T16:29:32.421204Z","shell.execute_reply.started":"2024-05-17T16:29:32.405968Z"},"metadata":{},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"wandb.login(key=\"62f8ddd1a44f05efc5c27f0ee5f22cf5bd70abc5\")\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]}]}