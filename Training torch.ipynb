{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-17T15:57:35.375095Z","iopub.status.busy":"2024-05-17T15:57:35.374741Z","iopub.status.idle":"2024-05-17T15:57:49.052140Z","shell.execute_reply":"2024-05-17T15:57:49.050842Z","shell.execute_reply.started":"2024-05-17T15:57:35.375065Z"},"metadata":{},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'Emotion-Classification'...\n","remote: Enumerating objects: 113, done.\u001b[K\n","remote: Counting objects: 100% (113/113), done.\u001b[K\n","remote: Compressing objects: 100% (84/84), done.\u001b[K\n","remote: Total 113 (delta 53), reused 84 (delta 24), pack-reused 0\u001b[K\n","Receiving objects: 100% (113/113), 270.72 KiB | 288.00 KiB/s, done.\n","Resolving deltas: 100% (53/53), done.\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","embedchain 0.1.102 requires sqlalchemy<3.0.0,>=2.0.27, but you have sqlalchemy 1.4.52 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!git clone https://github.com/AminMohamed-3/Emotion-Classification.git\n","!pip install transformers dataset accelerate -q\n","import sys\n","sys.path.append(\"/kaggle/working/Emotion-Classification\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:57:49.054802Z","iopub.status.busy":"2024-05-17T15:57:49.054440Z","iopub.status.idle":"2024-05-17T15:57:51.458996Z","shell.execute_reply":"2024-05-17T15:57:51.458062Z","shell.execute_reply.started":"2024-05-17T15:57:49.054770Z"},"metadata":{},"trusted":true},"outputs":[],"source":["import torch\n","from Training.dataset import prepare_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    DataCollatorForTokenClassification,\n","    Trainer,\n",")\n","import numpy as np\n","from config import NUM_LABELS\n","import wandb\n","from Training.utils import compute_metrics\n","from Training.utils import MultiLabelTrainer\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["# Define the model & Prepare Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"metadata":{}},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 211225/211225 [00:01<00:00, 108722.37it/s]\n","Map: 100%|██████████| 168980/168980 [00:25<00:00, 6537.72 examples/s]\n","Map: 100%|██████████| 21122/21122 [00:03<00:00, 6037.44 examples/s]\n","Map: 100%|██████████| 21123/21123 [00:03<00:00, 6781.23 examples/s]\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model_checkpoint = \"distilbert/distilroberta-base\"  # Using a larger model\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","dataset, id2label, label2id = prepare_dataset(tokenizer)\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint,\n","    num_labels=NUM_LABELS,\n","    id2label=id2label,\n","    label2id=label2id,\n","    problem_type=\"multi_label_classification\",\n",")\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{"metadata":{}},"source":["# Pytorch Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"metadata":{}},"outputs":[],"source":["train_dataloader = torch.utils.data.DataLoader(\n","    dataset[\"train\"].with_format(\"torch\"), batch_size=16, shuffle=True\n",")\n","val_dataloader = torch.utils.data.DataLoader(\n","    dataset[\"val\"].with_format(\"torch\"), batch_size=16, shuffle=True\n",")\n","test_dataloader = torch.utils.data.DataLoader(\n","    dataset[\"test\"].with_format(\"torch\"), batch_size=16, shuffle=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Model Class"]},{"cell_type":"code","execution_count":22,"metadata":{"metadata":{}},"outputs":[],"source":["class BERTClassifier(torch.nn.Module):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.model = model\n","\n","    def forward(self, **kwargs):\n","        return self.model(**kwargs, return_dict=False)\n","\n","\n","model_torch = BERTClassifier(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training hyperparameters\n","from torch.optim import AdamW\n","\n","LR = 2e-5\n","EPOCHS = 5\n","optimizer = AdamW(params=model.parameters(), lr=LR)\n","loss_fn = torch.nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import wandb\n","\n","# Initialize a new run\n","wandb.login(key=\"62f8ddd1a44f05efc5c27f0ee5f22cf5bd70abc5\")\n","wandb.init(project=\"Emotions\", name=\"naive_torch\")\n","# add wandb api key"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training loop\n","for epoch in range(EPOCHS):\n","    model_torch.train()\n","    for batch in tqdm(train_dataloader):\n","        optimizer.zero_grad()\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        outputs = model_torch(input_ids=input_ids, attention_mask=attention_mask)[0]\n","        loss = loss_fn(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Log training loss\n","        wandb.log({\"Train Loss\": loss.item()})\n","\n","    model_torch.eval()\n","    val_loss = 0\n","    val_acc = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            outputs = model_torch(input_ids=input_ids, attention_mask=attention_mask)[0]\n","            loss = loss_fn(outputs, labels)\n","            val_loss += loss.item()\n","            val_acc += (outputs.argmax(1) == labels).sum().item()\n","\n","            # Log validation loss\n","            wandb.log({\"Val Loss\": loss.item()})\n","\n","    val_loss /= len(val_dataloader)\n","    val_acc /= len(dataset[\"val\"])\n","    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n","    print(f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
